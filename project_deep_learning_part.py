# -*- coding: utf-8 -*-
"""Project_Deep Learning Part.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CVIiIPV1bU7ek7Igr0EO1Zo5Vp5aUPne
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_absolute_error
from sklearn.svm import SVC
from sklearn.inspection import permutation_importance
from scipy.stats import zscore
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split ,learning_curve
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PowerTransformer
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score


from sklearn.cluster import KMeans

"""# **Discovering The Data**"""

# Load the dataset
df = pd.read_csv('/content/Spotify-dataset.csv',low_memory=False)
pd.set_option('display.max_rows', None)

# Display dataset info
df.info()

print("\nPreview:")
print(df.head())

print("\nThe Shape: ", df.shape)

"""# **Cleaning Data**

## **Nulls**
"""

print("\nNull values per column:")
print(df.isnull().sum())
print('\n')
print("\nPercentage of nulls per column:")
print(df.isnull().mean() * 100)

numerical_cols = ['popularity', 'duration_ms', 'danceability', 'energy', 'loudness', 'speechiness',
                  'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'key', 'mode', 'time_signature']
for col in numerical_cols:
    if col in df.columns:
        df[col] = df[col].fillna(df[col].mean())

categorical_cols = ['track_genre', 'explicit', 'artists', 'album_name']
for col in categorical_cols:
    if col in df.columns:
        df[col] = df[col].fillna(df[col].mode()[0])

"""### **Dropping Nulls**"""

df.dropna(inplace=True)

"""## **Duplicates**"""

df.nunique()

"""### Normal Check"""

print("\nNumber of duplicate rows:", df.duplicated().sum())
df = df.drop_duplicates()

"""### Discovery"""

df.nunique()

"""**This tells us the following**

1.   Large Number of Duplicatted Ids
2.   Duplication in track_name after Checking with Unique Ids


"""

duplicate_subset = df.duplicated(subset=['track_id', 'track_name', 'artists']).sum()
print(f"Number of duplicates based on track_id, track_name, and artists: {duplicate_subset}")

# Sort by popularity (descending) to prioritize highest popularity
df.sort_values(by='popularity', ascending=False, inplace=True)

# Drop duplicates in place, keeping the first occurrence (highest popularity)
df.drop_duplicates(subset=['track_id', 'track_name', 'artists'], keep='first', inplace=True)

# Verify
print(f"Rows after dropping duplicates: {len(df)}")
print("Remaining duplicates:", df.duplicated(subset=['track_id', 'track_name', 'artists']).sum())

"""### **Advanced Checking For Duplicates**"""

# Step 1: Identify duplicates based on track_name and artists
duplicates = df[df.duplicated(subset=['track_name', 'artists'], keep=False)].copy()

# Step 2: Filter duplicates where at least one instance has popularity = 0
duplicates['is_zero_popularity'] = duplicates['popularity'] == 0
grouped = duplicates.groupby(['track_name', 'artists'])

# Step 3: Find duplicates with both popularity = 0 and higher popularity
problematic_duplicates = []
numerical_cols = ['duration_ms', 'danceability', 'energy', 'loudness', 'speechiness',
                  'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']
indices_to_drop = []
for (track_name, artists), group in grouped:
    if group['popularity'].eq(0).any() and group['popularity'].gt(0).any():
        group = group.sort_values('popularity', ascending=False)
        max_pop_row = group[group['popularity'] > 0].iloc[0]
        zero_pop_rows = group[group['popularity'] == 0]
        track_ids_info = []
        for idx, zero_row in zero_pop_rows.iterrows():
            diffs = {col: abs(max_pop_row[col] - zero_row[col]) for col in numerical_cols}
            significant_diffs = {col: diff for col, diff in diffs.items() if diff > 0.05 * abs(max_pop_row[col])}
            track_ids_info.append({
                'track_id': zero_row['track_id'],
                'popularity': zero_row['popularity'],
                'significant_differences': str(significant_diffs) if significant_diffs else 'None '
            })
            # Mark rows with no significant differences for dropping
            if not significant_diffs:
                indices_to_drop.append(idx)
        # Add the highest popularity track_id
        track_ids_info.append({
            'track_id': max_pop_row['track_id'],
            'popularity': max_pop_row['popularity'],
            'significant_differences': 'Reference'
        })
        problematic_duplicates.append({
            'track_name': track_name,
            'artists': artists,
            'track_ids_info': track_ids_info
        })

# Step 4: Display first 10 problematic duplicates in a concise format
if problematic_duplicates:
    print("First 10 duplicates with popularity = 0 and higher popularity:")
    for entry in problematic_duplicates[:10]:
        print(f"\nSong: {entry['track_name']} by {entry['artists']}")
        print("\nTrack IDs and differences:")
        for info in entry['track_ids_info']:
            print(f"  track_id: {info['track_id']}, popularity: {info['popularity']}, differences: {info['significant_differences']}")
else:
    print("No duplicates found with popularity = 0 and higher popularity.")

# Step 5: Drop duplicates with popularity = 0 and no significant differences
df.drop(indices_to_drop, inplace=True)

# Step 6: Check After Dropping
print(f"Rows after dropping duplicates: {len(df)}")

"""## **Outliers**

### **Displaying Outliers**
"""

# all numerical columns
numerical_cols = [col for col in df.columns if df[col].dtype in ['int64', 'float64']]
cols_per_row = 3
rows_needed = (len(numerical_cols) + cols_per_row - 1) // cols_per_row

numerical_cols_for_outliers = [col for col in numerical_cols if col not in ['popularity', 'duration_ms']]

# Subplot
fig, axes = plt.subplots(nrows=rows_needed, ncols=cols_per_row, figsize=(15, rows_needed * 4))
axes = axes.flatten()

# boxplot for each column
for i, col in enumerate(numerical_cols):
    sns.boxplot(x=df[col], ax=axes[i])
    axes[i].set_title(f'Boxplot of {col}')
    axes[i].grid(False)

# hiding extra axes if less charts present
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""### **Handling**"""

for col in numerical_cols_for_outliers:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_limit = Q1 - 1.5 * IQR
    upper_limit = Q3 + 1.5 * IQR
    df[col] = np.where(df[col] < lower_limit, lower_limit, df[col])
    df[col] = np.where(df[col] > upper_limit, upper_limit, df[col])

fig, axes = plt.subplots(nrows=rows_needed, ncols=cols_per_row, figsize=(15, rows_needed * 4))
axes = axes.flatten()


for i, col in enumerate(numerical_cols):
    sns.boxplot(x=df[col], ax=axes[i])
    axes[i].set_title(f'Boxplot of {col}')
    axes[i].grid(False)

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""We kept Pouplarity Outliers as it is a factor in how the model should predict the outcome (Normally some songs are far better and more popular than others )

# **Data Analysis and Visualization**
"""

df['track_genre'].unique()

genre_popularity_df =  df.groupby(['track_genre'])['popularity'].mean().sort_values().reset_index(name='popularity_mean')
genre_popularity_df.tail()

plt.figure(figsize=(12, 25))

sns.barplot(
    data=genre_popularity_df,
    y='track_genre',
    x='popularity_mean',
)
plt.title("Popularity with genre")
plt.show()

plt.figure(figsize=(12, 10))
sns.heatmap(df[['popularity', 'danceability', 'energy', 'acousticness' , 'key' , 'loudness','speechiness' , 'instrumentalness','liveness','valence','tempo']].corr(),
             annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)
plt.title('Correlation Heatmap of Selected Features', fontsize=14, pad=15)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
plt.tight_layout()
plt.show()

df['loudness'].describe()

bins = [-20, -15, -10, -5, 0, 5]
labels = ['-20 to -15', '-15 to -10', '-10 to -5', '-5 to 0', '0 to 5']

df['loudness_bins'] = pd.cut(df['loudness'], bins=bins, labels=labels)


sns.barplot(x='loudness_bins', y='popularity', data=df, estimator='mean')
plt.title('Average Popularity by Loudness Range')
plt.ylabel('Average Popularity')
plt.xlabel('Loudness (dB)')
plt.show()

top_genres = df['track_genre'].value_counts().head(8).index
genre_data = df[df['track_genre'].isin(top_genres)]

fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('Audio Features Across Top Music Genres', fontsize=18, fontweight='bold')

features = ['danceability', 'energy', 'valence', 'acousticness']
colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']

for i, feature in enumerate(features):
    row = i // 2
    col = i % 2

    box_data = [genre_data[genre_data['track_genre'] == genre][feature].dropna() for genre in top_genres]
    bp = axes[row, col].boxplot(box_data, labels=top_genres, patch_artist=True)

    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    axes[row, col].set_title(f'{feature.title()} by Genre', fontsize=14, fontweight='bold')
    axes[row, col].set_ylabel(feature.title(), fontsize=12)
    axes[row, col].tick_params(axis='x', rotation=45)
    axes[row, col].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""# **Feature Engineering**"""

def key_cyclic_encoding() -> None:
    """Encoding the 'key' column and add two columns in dataframe."""
    df['key_sin'] = np.sin(2 * np.pi * df['key']/12)
    df['key_cos'] = np.cos(2 * np.pi * df['key']/12)

    df['key_mode'] = df['key'].astype('str') + df['mode'].astype('str')

    print("Columns formed by 'key' column : ", [col for col in df.columns if col.startswith('key')], "\n")

key_cyclic_encoding()

def classify_duration(z):
    """
    Classify the log-duration based on the different classes.
    We used the standardized (Z-score) method.
    """
    if z < -0.3:
        return 'short'
    elif -0.3 <= z < 0.3:
        return 'normal'
    elif 0.3 <= z < 1:
        return 'mid_normal'
    elif 1 <= z < 2:
        return 'long'
    else:
        return 'very_long'

def feature_engineering_on_duration_col() -> None:
    """Add a minute column, class column, and one-hot encoding."""

    if 'duration_ms' in df.columns:
        # Minute Column
        df['duration_min'] = df['duration_ms'] / 60_000

        # Log transformation
        df['duration_log'] = np.log1p(df['duration_min'])

        # Standardize log-duration
        scaler = StandardScaler()
        df['duration_class'] = df['duration_log_z'] = scaler.fit_transform(df[['duration_log']])

        # Make a class
        df['duration_class'] = df['duration_log_z'].apply(classify_duration)

    print("Columns formed by 'duration' column : ", [col for col in df.columns if col.startswith('duration')], "\n")

feature_engineering_on_duration_col()

def feature_engineering_on_time_signature_col():
    """
    1. Make class (1 -> Common, 0 -> Others)
    """
    df['time_signature_class_boolean'] = df['time_signature'].apply(lambda x : 1 if x in [3, 4] else 0)

    print("Columns formed by duration column : ", [col for col in df.columns if col.startswith('time_signature')], "\n")

feature_engineering_on_time_signature_col()

def feature_engineering_on_loudness_col():
    pt = PowerTransformer(method='yeo-johnson')
    df['loudness_yeo'] = pt.fit_transform(df[['loudness']])

    df['loudness_level'] = pd.qcut(
        df['loudness_yeo'],
        q=5,
        labels=['Very Quiet', 'Quiet', 'Normal', 'Loud', 'Very Loud']
    )

    df['loudness_intensity'] = df['energy'] * (df['loudness'] + (abs(df['loudness']).min()))


    print("Columns formed by 'loudness' column : ", [col for col in df.columns if col.startswith('loudness')], "\n")

feature_engineering_on_loudness_col()

def feature_engineering_on_multicolumn_col():
    df['is_instrumental'] = (df['instrumentalness'] > 0.8).astype('int')
    df['is_dance_hit'] = ((df['danceability'] > 0.7840) & (df['energy'] > 0.9410)).astype('int')

    df['happy_dance'] = df['valence'] * df['danceability']
    df['acoustics_instrumental'] = df['instrumentalness'] * df['acousticness']

    print("Columns formed by multi column : ", [col for col in df.columns if col.startswith('is')], \
        "['happy_dance', , 'acoustics_instrumental'] \n" )

feature_engineering_on_multicolumn_col()

def feature_engineering_on_popularity_col():
    df['popularity_level'] =  pd.qcut(
    df['popularity'],
    q=[0, 0.5, 0.8, 0.9, 0.97, 1.0],
    labels=['low', 'normal', 'medium', 'high', 'very high']
    )

    print("Columns formed by 'popularity' column : ", [col for col in df.columns if col.startswith('popularity')], "\n" )

feature_engineering_on_popularity_col()

def feature_engineering_on_artist_col():

    artists_popularity =  df.groupby(['artists'])['popularity'].mean()
    df['artists_avg_popularity'] = df['artists'].map(artists_popularity)

    df['artist_song_count'] = df['artists'].map(df['artists'].value_counts())

    print("Columns formed by 'artist' column : ", [col for col in df.columns if col.startswith('artist')], "\n" )


def feature_engineering_on_album_col():

    df['album_freq'] = df['album_name'].map(df['album_name'].value_counts())

    print("Columns formed by 'album' column : ", [col for col in df.columns if col.startswith('album')], "\n" )

feature_engineering_on_artist_col()

def feature_engineering_on_tempo_col():

    df['tempo_class'] = pd.cut(
        df['tempo'],
        bins=[0, 40, 80, 180, 210, float('inf')],
        labels=['very slow', 'slow', 'normal', 'fast', 'very fast'])

    df['temp_zscore'] = (df['tempo'] - df['tempo'].mean()) / (df['tempo'].std())

    df['tempo_vs_genre'] = df['tempo'] - df.groupby('track_genre')['tempo'].transform('mean')

    print("Columns formed by 'tempo' column : ", [col for col in df.columns if col.startswith('temp')], "\n" )


def feature_engineering_on_energy_col():

    df['energy_rank_pct'] = df['energy'].rank(pct=True)

    df['loud_energy_ratio'] = (df['loudness'] + 50) / (df['energy'] + 1e-6)

    print("Columns formed by 'energy' column : ", [col for col in df.columns if 'energy' in col], "\n" )

feature_engineering_on_tempo_col()

def pca_columns():
    pca = PCA(n_components=1)
    df['mood_pca'] = pca.fit_transform(df[['valence', 'energy', 'danceability']])

    print("Columns formed by 'pca' : ['mood_pca'] \n" )

pca_columns()

def drop_col(cols):
    for col in cols:
        if col in df.columns:
            df.drop(columns=[col], inplace=True)
            print(f"'{col}' is drop.\n")
    print(f"Drop Columns are : {cols}", flush=True)

le = LabelEncoder()
df['track_genre'] = le.fit_transform(df['track_genre'])

df['explicit'] = df['explicit'].astype(int)
drop_col(['index', 'track_id','album_name', 'track_name','artists','loudness_bins','duration_ms','key_mode'])

loudness_mapping = {'Very Quiet': 0, 'Quiet': 1, 'Normal': 2, 'Loud': 3, 'Very Loud': 4}
df['loudness_level'] = df['loudness_level'].map(loudness_mapping).astype(int)

tempo_mapping = {'very slow': 0, 'slow': 1, 'normal': 2, 'fast': 3, 'very fast': 4}
df['tempo_class'] = df['tempo_class'].map(tempo_mapping).astype(int)

pop_mapping = {'low': 0, 'normal': 1, 'medium': 2, 'high': 3, 'very high': 4}
df['popularity_level_le'] = df['popularity_level'].map(pop_mapping).astype(int)

le = LabelEncoder()
df['duration_class'] = le.fit_transform(df['duration_class'])

"""### THE Results"""

df.info()

pd.set_option('display.max_columns', None)
df.head()

df.isnull().sum()

"""# **Building Models**

## **Regression**

### Linear Regression
"""

df_new=df.copy()
df_new.drop(columns=['track_genre','popularity_level'], inplace=True)

# Create X(features), y(target)
X = df_new.drop(columns=['popularity'])
y = df_new['popularity']

# Train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=82)
print(f"{'--'*15}  Shapes  {'--'*15}")
print(f"X --> trains size : {X_train.shape} test size : {X_test.shape}")
print(f"y --> trains size : {y_train.shape} test size : {y_test.shape} \n")

linear_model = LinearRegression()

linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# calculate the predict the model
y_pred = linear_model.predict(X_test)
y_pred_train = linear_model.predict(X_train)

train_score = linear_model.score(X_train, y_train)
test_score = linear_model.score(X_test, y_test)

print(f"{'--'*15}  Model Scores  {'--'*15}")
print(f"Training R² Score: {train_score}")
print(f"Testing R² Score: {test_score}")

print(f"{'--'*30}  \n")
print(f"{'--'*30}  \n")



"""### **Deep Learning Regression**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# Step 7.1: Define features and target for regression
features = ['track_genre', 'duration_class', 'popularity_level_le',
            'loudness_level', 'tempo_class',
            'danceability', 'energy', 'loudness', 'speechiness', 'acousticness',
            'instrumentalness', 'liveness', 'valence', 'tempo',
            'key_sin', 'key_cos', 'duration_log', 'duration_log_z',
            'temp_zscore', 'loudness_intensity', 'happy_dance', 'acoustics_instrumental',
            'artists_avg_popularity', 'artist_song_count', 'album_freq',
            'tempo_vs_genre', 'energy_rank_pct', 'loud_energy_ratio', 'loudness_yeo',
            'explicit', 'mode', 'is_instrumental', 'is_dance_hit']
target = 'popularity'

# Ensure all features exist in DataFrame
features = [f for f in features if f in df.columns]

# Step 7.2: Prepare data for regression
X = df[features]
y = df[target]

# Step 7.3: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 7.4: Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 7.5: Define and compile deep learning model
model = Sequential([
    Dense(256, input_shape=(X_train_scaled.shape[1],), kernel_regularizer=l2(0.001)),
    LeakyReLU(alpha=0.01),
    Dropout(0.3),
    Dense(128,kernel_regularizer=l2(0.001)),
    LeakyReLU(alpha=0.01),
    Dropout(0.3),
    Dense(64, kernel_regularizer=l2(0.001)),
    LeakyReLU(alpha=0.01),
    Dense(1)  # Output layer for regression
])

model.compile(optimizer=Adam(), loss='mse', metrics=['mae','r2_score'])

# Step 7.6: Train the model with early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
history = model.fit(X_train_scaled, y_train,
                    validation_data=(X_test_scaled, y_test),
                    epochs=200,
                    batch_size=32,
                    callbacks=[early_stopping],
                    verbose=1)

# Step 7.7: Make predictions
y_train_pred = model.predict(X_train_scaled).flatten()
y_test_pred = model.predict(X_test_scaled).flatten()

# Step 7.8: Evaluate model with MSE, MAE, and R²
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Step 7.9: Print evaluation metrics
print(f"\nTraining MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, R²: {train_r2:.4f}")
print(f"Testing MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, R²: {test_r2:.4f}")

# Plot training vs validation loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.title('Learning Curve (MSE)')
plt.xlabel('Epochs'); plt.ylabel('Loss')
plt.legend(); plt.grid(True); plt.show()

# Plot MAE (accuracy proxy in regression)
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Test MAE')
plt.title('Mean Absolute Error Over Epochs')
plt.xlabel('Epochs'); plt.ylabel('MAE')
plt.legend(); plt.grid(True); plt.show()

"""## **Classification**

### Deep Learning
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau ,LearningRateScheduler
from tensorflow.keras.utils import to_categorical
from imblearn.over_sampling import SMOTE
from scipy.stats import spearmanr
from sklearn.utils.class_weight import compute_class_weight

# Learning rate warm-up scheduler
def lr_warmup(epoch, lr):
    warmup_epochs = 10
    initial_lr = 0.0001
    target_lr = 0.001
    if epoch < warmup_epochs:
        return initial_lr + (target_lr - initial_lr) * epoch / warmup_epochs
    return lr

# Define all numerical features (excluding popularity_level)
features = [col for col in df.columns if df[col].dtype in ['int64', 'float64', 'int32'] and col not in ['popularity_level', 'popularity_level_le']]
target = 'popularity_level_le'



X = df[features]
y = df[target]

# Convert target to categorical for multiclass classification
num_classes = len(df[target].unique())
y_categorical = to_categorical(y, num_classes=num_classes)


# Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
y_resampled_categorical = to_categorical(y_resampled, num_classes=num_classes)
print(f"Dataset size after SMOTE: {len(X_resampled)} rows")

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled_categorical, test_size=0.2, random_state=42, stratify=y_resampled_categorical)


# Standardize
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define k-fold cross-validation
n_splits = 2
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize lists to store metrics
train_accuracies = []
val_accuracies = []
histories = []
best_val_acc = 0
best_fold = 0
best_history = None

# Define model creation function
def create_model(input_shape, num_classes):
    model = Sequential([
        Dense(128, input_shape=(input_shape,), kernel_regularizer=l2(0.01)),
        BatchNormalization(),
        LeakyReLU(alpha=0.01),
        Dropout(0.25),
        Dense(64, kernel_regularizer=l2(0.01)),
        BatchNormalization(),
        LeakyReLU(alpha=0.01),
        Dropout(0.25),
        Dense(32, kernel_regularizer=l2(0.01)),
        LeakyReLU(alpha=0.01),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer=Adam(learning_rate=0.0001),  # Initial LR for warm-up
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model


# Cross-validation training
for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_scaled, np.argmax(y_train, axis=1))):
    print(f"\nTraining Fold {fold + 1}/{n_splits}")
    X_fold_train, X_fold_val = X_train_scaled[train_idx], X_train_scaled[val_idx]
    y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]

    # Create model
    model = create_model(X_train_scaled.shape[1], num_classes)

    # Define callbacks
    early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)
    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)
    lr_warmup_scheduler = LearningRateScheduler(lr_warmup, verbose=1)

    # Train
    history = model.fit(X_fold_train, y_fold_train,
                        validation_data=(X_fold_val, y_fold_val),
                        epochs=200,
                        batch_size=128,
                        callbacks=[early_stopping, lr_warmup_scheduler, lr_scheduler],
                        verbose=1)

    # Store history
    histories.append(history)

    # Evaluate
    train_loss, train_acc = model.evaluate(X_fold_train, y_fold_train, verbose=0)
    val_loss, val_acc = model.evaluate(X_fold_val, y_fold_val, verbose=0)
    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)

    print(f"Fold {fold + 1} - Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}")

    # Track best fold
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_fold = fold + 1
        best_history = history
        best_train_acc = train_acc

    # For the last fold, evaluate on test set
    if fold == n_splits - 1:
        y_test_pred = model.predict(X_test_scaled)
        y_test_pred_classes = np.argmax(y_test_pred, axis=1)
        y_test_true = np.argmax(y_test, axis=1)
        test_accuracy = accuracy_score(y_test_true, y_test_pred_classes)

print(f"Model Train Accuracy: {best_train_acc:.4f}")
print(f"Model Test Accuracy: {test_accuracy:.4f}")
print("\nClassification Report (Test Set, Last Fold):")
print(classification_report(y_test_true, y_test_pred_classes, target_names=['low', 'normal', 'medium', 'high', 'very high']))

# Plot confusion matrix
cm = confusion_matrix(y_test_true, y_test_pred_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['low', 'normal', 'medium', 'high', 'very high'], yticklabels=['low', 'normal', 'medium', 'high', 'very high'])
plt.title('Confusion Matrix (Test Set, Last Fold)')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Plot learning curve
plt.figure(figsize=(10, 5))
plt.plot(best_history.history['loss'], label='Train Loss')
plt.plot(best_history.history['val_loss'], label='Validation Loss')
plt.title(f'Learning Curve (Loss, Fold {best_fold})')
plt.xlabel('Epochs')
plt.ylabel('Categorical Crossentropy Loss')
plt.legend()
plt.grid(True)
plt.show()

# Plot training history (accuracy)
plt.figure(figsize=(10, 5))
plt.plot(best_history.history['accuracy'], label='Train Accuracy')
plt.plot(best_history.history['val_accuracy'], label='Validation Accuracy')
plt.title(f'Training History (Accuracy, Fold {best_fold})')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()